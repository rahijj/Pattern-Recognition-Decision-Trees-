# -*- coding: utf-8 -*-
"""Assign_2_Pattern.ipynb

Automatically generated by Colaboratory.

"""

import math
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.patches as mpatches
from scipy.stats import norm
from numpy.linalg import norm, inv
import matplotlib.pyplot as plt 
import matplotlib.tri as tri
from sklearn.metrics import confusion_matrix, mean_squared_error
from numpy import cov
import tensorflow as tf
from statistics import stdev, mean 
import csv
from copy import deepcopy
from pprint import pprint
import random
from sklearn.model_selection import RepeatedKFold
from sklearn import preprocessing

# Data Extraction

Tic_tac_data = pd.read_csv("tic-tac-toe.data")
tic_att = ['top-left', 'top-middle', 'top-right', 'middle-left', 'middle-middle',
      'middle-right', 'bottom-left', 'bottom-middle', 'bottom-right', 'Class' ]
Tic_tac_data.columns = tic_att
TIC_TAC_DATA = Tic_tac_data.to_numpy()

wine_data = pd.read_csv("wine.data")
wine_att = ['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 
            'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 
            'Proanthocyanins', 'Color intensity', 'Hue',
            'OD280/OD315 of diluted wines', 'Proline']
wine_data.columns = wine_att 
WINE_DATA = wine_data.to_numpy()

# train_data_tic, test_data_tic = train_test_split(Tic_tac_data, test_size=0.2)

# train_data_wine, test_data_wine = train_test_split(wine_data, test_size=0.2)

# Check if only one kind of Label exists for all data
def purity_check(data, label_col_num):
  c = np.unique(data[:, label_col_num])
  if (len(c) == 1):
    return True
  else:
    return False

# Generate all possible splits for the WINE dataset
def get_poss_splits_wine(data, label_col_num):
  poss = {}
  for i in range(1, data.shape[1], 1):
    temp = []
    u, u_ind = np.unique(data[:, i], return_index=True)
    l = len(u)
    for j in range(l-1):
      if data[u_ind[j], label_col_num]!=data[u_ind[j+1], label_col_num]:
        temp.append((u[j]+u[j+1]) / 2)
    if len(temp)!=0:
      poss[i] = temp
  
  return poss

# Generate all possible splits for the Tic-Tac dataset
def get_poss_splits_tic(data):
  poss = {}
  for i in range(data.shape[1]-1):
    temp = []
    u = np.unique(data[:, i])
    l = len(u)
    for j in range(l):
      temp.append(u[j])
    poss[i] = temp
  return poss

# Split the dataset based on the chosen attribute and split point(in case of continuous dataset only)
def split(data, split_att, split_point, dat_type):

  if dat_type == 1:
    dat_below = data[(data[:, split_att] <= split_point)]
    dat_above = data[(data[:, split_att] > split_point)]  
    return dat_below, dat_above
  
  else:
    x_data = data[(data[:, split_att] == 'x')]
    o_data = data[(data[:, split_att] == 'o')]
    b_data = data[(data[:, split_att] == 'b')]

    return x_data, o_data, b_data

# Calculate Split Info for the continouous dataset for calculating Gain Ratio
def get_split_info_cont(dat_below, dat_above):
  
  L = len(dat_below) + len(dat_above)
  P_dat_below = len(dat_below)/ L
  P_dat_above = len(dat_above) / L

  if P_dat_below == 0:
    split_info = -1 * P_dat_above * -np.log2(P_dat_above)
  elif P_dat_above == 0:
    split_info = -1 * P_dat_below * np.log2(P_dat_below)
  else:
    split_info = -1 * (P_dat_below * np.log2(P_dat_below) + P_dat_above * np.log2(P_dat_above) )
  
  return split_info

# Calculate Split Info for the Tic-Tac dataset for calculating Gain Ratio
def get_split_info_tic(x_data, o_data, b_data):
  
  len_x = len(x_data)
  len_o = len(o_data)
  len_b = len(b_data)

  L = len_x + len_o + len_b

  P_x_data = len_x / L
  P_o_data = len_o / L
  P_b_data = len_b / L

  split_info = 1
  
  if P_x_data == 0:
    x_info = 0
  else:
    x_info = P_x_data * np.log2(P_x_data)
  
  if P_o_data == 0:
    o_info = 0
  else:
    o_info = P_o_data * np.log2(P_o_data)
  
  if P_b_data == 0:
    b_info = 0
  else:
    b_info = P_b_data * np.log2(P_b_data)
  
  if (x_info + o_info + b_info)!= 0:
    split_info = -1 * (x_info + o_info + b_info)
  
  return split_info

def get_entropy(att_col):
  c = np.unique(att_col, return_counts=True)
  t = sum(c[1])
  P = c[1] / t
  H = np.sum(P * -np.log2(P))
  return H

def get_total_entropy(dat_below, dat_above, label_col_num):
  
  len_DA = len(dat_above)
  len_DB = len(dat_below)
  L = len_DA + len_DB
  P_dat_below = len_DB / L
  P_dat_above = len_DA / L

  total_ent = ((P_dat_below * get_entropy(dat_below[:, label_col_num])) + 
               (P_dat_above * get_entropy(dat_above[:, label_col_num])))
  return total_ent

def get_total_entropy_tic(x_data, o_data, b_data, label_col_num):
  
  len_x = len(x_data)
  len_o = len(o_data)
  len_b = len(b_data)
  L = len_x + len_o + len_b
  P_x_data = len_x / L
  P_o_data = len_o / L
  P_b_data = len_b / L

  if P_x_data == 0:
    X_getEnt = 0
  else:
    X_getEnt = get_entropy(x_data[:, label_col_num])
  
  if P_o_data == 0:
    O_getEnt = 0
  else:
    O_getEnt = get_entropy(o_data[:, label_col_num])
  
  if P_b_data == 0:
    B_getEnt = 0
  else:
    B_getEnt = get_entropy(b_data[:, label_col_num])
  
  total_ent = ((P_x_data * X_getEnt) + 
               (P_o_data * O_getEnt) +
               (P_b_data * B_getEnt))
  return abs(total_ent)

# Calculate the best possible split attribute and value based on Information Gain
def best_split(data, possible_splits, label_col_num, dat_type):
  
  current_ent = get_entropy(data[:, label_col_num])
  best_info_gain = 0
  best_split_col = -1
  best_split_val = -1

  if dat_type == 0:
    
    for i in possible_splits:
      x_data, o_data, b_data = split(data, i, -1, dat_type) 
      new_total_ent = get_total_entropy_tic(x_data, o_data, b_data, label_col_num )
      info_gain = current_ent - new_total_ent

      if (info_gain >= best_info_gain):
        best_info_gain = info_gain
        best_split_col = i
    return best_split_col
    
  else:
    
    for i in possible_splits:
      for j in possible_splits[i]:
        dat_below, dat_above = split(data, i, j, dat_type) 
        new_total_ent = get_total_entropy(dat_below, dat_above, label_col_num )
        info_gain = current_ent - new_total_ent
        
        if (info_gain >= best_info_gain):
          best_info_gain = info_gain
          best_split_col = i
          best_split_val = j
    
    return best_split_col, best_split_val

# Calculate the best possible split attribute and value based on Gain-Ratio
def best_split_gain_ratio(data, possible_splits, label_col_num, dat_type):
  
  current_ent = get_entropy(data[:, label_col_num])
  best_gain_ratio = 0
  best_split_col = -1
  best_split_val = -1

  
  if dat_type == 0:
    
    for i in possible_splits:
      x_data, o_data, b_data = split(data, i, -1, dat_type) 
      new_total_ent = get_total_entropy_tic(x_data, o_data, b_data, label_col_num )
      info_gain = current_ent - new_total_ent
      gain_ratio = info_gain / get_split_info_tic(x_data, o_data, b_data)
      
      if (gain_ratio >= best_gain_ratio):
        best_gain_ratio = gain_ratio
        best_split_col = i

    return best_split_col
    
  else:
    
    for i in possible_splits:
      for j in possible_splits[i]:
        dat_below, dat_above = split(data, i, j, dat_type) 
        new_total_ent = get_total_entropy(dat_below, dat_above, label_col_num )
        info_gain = current_ent - new_total_ent
        gain_ratio = info_gain / get_split_info_cont(dat_below, dat_above)
       
        if (gain_ratio >= best_gain_ratio):
          best_gain_ratio = gain_ratio
          best_split_col = i
          best_split_val = j
    
    return best_split_col, best_split_val

# CLASS NODE

class Node:
  
  def __init__(self, leaf = False, pred=None, feature=None, threshold=None,
               left=None, right=None, x=None, o=None, b=None ):
    self.Leaf = leaf
    self.Pred = pred
    self.Feature = feature
    self.Threshold = threshold
    self.Left = left
    self.Right = right
    self.X = x
    self.O = o
    self.B = b

# ID3 Algorithm that creates the Tree using Information gain
# Continuous Data: dat_type = 1, Categorical Data: data_type = 0
def ID3(exclude_col, data, label_col_num, dat_type, curr_dep=0):
 
  dep = curr_dep
  if curr_dep ==0:
    exclude_col = []

  n = Node()
  classes, class_count = np.unique(data[:, label_col_num], return_counts=True)
  v = classes[class_count.argmax()]

  if purity_check(data, label_col_num):
    setattr(n, 'Pred', v)
    setattr(n, 'Leaf', True)
    
    return n
  
  else:
    # If continuous data
    if dat_type == 1:
      dep = dep+1
      possible_splits = get_poss_splits_wine(data, label_col_num)
      if len(possible_splits)==0:
        setattr(n, 'Pred', v)
        setattr(n, 'Leaf', True)
    
        return n

      best_split_col, best_split_val = best_split(data, possible_splits, label_col_num, dat_type)
      dat_below, dat_above = split(data, best_split_col, best_split_val, dat_type)

      setattr(n, 'Feature', best_split_col)
      setattr(n, 'Threshold', best_split_val)

      if dat_below.shape[0] != 0:
        setattr(n, 'Left', ID3([], dat_below, label_col_num, dat_type, dep))
      else:
        setattr(n, 'Left', Node(leaf=True, pred=v))
      if dat_above.shape[0] != 0:  
        setattr(n, 'Right', ID3([], dat_above, label_col_num, dat_type, dep))
      else:
        setattr(n, 'Right', Node(leaf=True, pred=v))
      return n
    
    # If categorical data
    else:
      dep +=1
      possible_splits = get_poss_splits_tic(data)
     
      if len(exclude_col)==len(possible_splits) or len(possible_splits)==0:
        setattr(n, 'Pred', v)
        setattr(n, 'Leaf', True)
     
        return n
     
      for i in exclude_col:
        possible_splits.pop(i, None)
      
      best_split_col = best_split(data, possible_splits, label_col_num, dat_type)
      x_data, o_data, b_data = split(data, best_split_col, -1, dat_type)
      exclude_col.append(best_split_col)
          
      setattr(n, 'Feature', best_split_col)
      
      if x_data.shape[0] != 0:
        ex = deepcopy(exclude_col)
        setattr(n, 'X', ID3(ex, x_data, label_col_num, dat_type, dep))
      else:
        setattr(n, 'X', Node(leaf=True, pred=v))
      
      if o_data.shape[0] != 0:  
        ex1 = deepcopy(exclude_col)
        setattr(n, 'O', ID3(ex1, o_data, label_col_num, dat_type, dep))
      else:
        setattr(n, 'O', Node(leaf=True, pred=v))
      
      if b_data.shape[0] != 0:    
        ex2 = deepcopy(exclude_col)
        setattr(n, 'B', ID3(ex2, b_data, label_col_num, dat_type, dep))
      else:
        setattr(n, 'B', Node(leaf=True, pred=v))
    
      return n

# ID3 Algorithm that creates the Tree using Gain Ratio
# Continuous Data: dat_type = 1, Categorical Data: data_type = 0
def ID3_gain_ratio(exclude_col, data, label_col_num, dat_type, curr_dep=0):

  dep = curr_dep

  if curr_dep ==0:
    exclude_col = []

  n = Node()

  classes, class_count = np.unique(data[:, label_col_num], return_counts=True)
  v = classes[class_count.argmax()]  

  if purity_check(data, label_col_num):
    setattr(n, 'Pred', v)
    setattr(n, 'Leaf', True)

    return n
  
  else:
    # If continuous data
    if dat_type == 1:
      dep = dep+1
      possible_splits = get_poss_splits_wine(data, label_col_num)

      if len(possible_splits)==0:
        setattr(n, 'Pred', v)
        setattr(n, 'Leaf', True)

        return n

      best_split_col, best_split_val = best_split_gain_ratio(data, possible_splits, label_col_num, dat_type)
      dat_below, dat_above = split(data, best_split_col, best_split_val, dat_type)

      setattr(n, 'Feature', best_split_col)
      setattr(n, 'Threshold', best_split_val)

      if dat_below.shape[0] != 0:
        setattr(n, 'Left', ID3_gain_ratio([], dat_below, label_col_num, dat_type, dep))
      else:
        setattr(n, 'Left', Node(leaf=True, pred=v))
      if dat_above.shape[0] != 0:    
        setattr(n, 'Right', ID3_gain_ratio([], dat_above, label_col_num, dat_type, dep))
      else:
        setattr(n, 'Right', Node(leaf=True, pred=v))
      
      return n
    
    # Id categorical data
    else:
      dep +=1
      possible_splits = get_poss_splits_tic(data)

      if len(exclude_col)==len(possible_splits) or len(possible_splits)==0 :
        setattr(n, 'Pred', v)
        setattr(n, 'Leaf', True)

        return n
     
      for i in exclude_col:
        possible_splits.pop(i, None)
      
      best_split_col = best_split_gain_ratio(data, possible_splits, label_col_num, dat_type)
      x_data, o_data, b_data = split(data, best_split_col, -1, dat_type)
      exclude_col.append(best_split_col)

      setattr(n, 'Feature', best_split_col)
      
      if x_data.shape[0] != 0:
        ex = deepcopy(exclude_col)
        setattr(n, 'X', ID3_gain_ratio(ex, x_data, label_col_num, dat_type, dep))
      else:
        setattr(n, 'X', Node(leaf=True, pred=v))
      
      if o_data.shape[0] != 0: 
        ex1 = deepcopy(exclude_col)   
        setattr(n, 'O', ID3_gain_ratio(ex1, o_data, label_col_num, dat_type, dep))
      else:
        setattr(n, 'O', Node(leaf=True, pred=v))
      
      if b_data.shape[0] != 0:    
        ex2 = deepcopy(exclude_col)
        setattr(n, 'B', ID3_gain_ratio(ex2, b_data, label_col_num, dat_type, dep))
      else:
        setattr(n, 'B', Node(leaf=True, pred=v))
    
      return n

def predict(node, point):

  if node.Leaf:
    return node.Pred

  else:
    if point[node.Feature] <= node.Threshold:
      return predict(node.Left, point)
    else:
      return predict(node.Right, point)

def predict_tic(node, point):

  if node.Leaf:
    return node.Pred
  else:
    
    if point[node.Feature] == 'x':
      return predict_tic(node.X, point)
    
    elif point[node.Feature] == 'o':
      return predict_tic(node.O, point)
    
    elif point[node.Feature] == 'b':
      return predict_tic(node.B, point)


# Add Noise to Attributes of Wine Dataset
# percent ranges from 0 to 1

def Add_noise_cont(data, percent):
  l = len(data)
  noise_per = round(percent * l)
  ran = list(range(l))
  rand_samp = random.sample(ran, noise_per)

  for i in rand_samp:
    noise = np.random.normal(0, 1, data.shape[1]-1)
    data[i, 1:] = data[i, 1:] + noise
  
  return data

# Add Noise to Attributes of Tic-Tac Dataset
# percent ranges from 0 to 1
def Add_noise_cat(data, percent):
  l = len(data)
  noise_per = round(percent * l)
  ran = list(range(l))
  rand_samp = random.sample(ran, noise_per)

  for i in rand_samp:
    noise = list(map(random.choice,[['x','o','b']]*(data.shape[1]-1)))
    data[i, :-1] = deepcopy(noise)
  
  return data

# Add Contradictory Class Noise to Wine Dataset
def Add_contrad_noise_wine(data, percent):
  l = len(data)
  noise_per = round(percent * l)
  ran = list(range(l))
  data_n = []
  rand_samp = random.sample(ran, noise_per)
  for i in rand_samp:
    lab = data[i,0]
    if lab == 1:
      lab = random.choice([2,3])
    elif lab == 2:
      lab = random.choice([1,3])
    elif lab == 3:
      lab =random.choice([1,2])
    
    data_n.append(deepcopy(data[i,:]))
    data[i,0] = lab

  data_r = np.append(data, data_n, axis=0)
  np.random.shuffle(data_r)

  return data_r

# Add Contradictory Class Noise to Tic-Tac Dataset
def Add_contrad_noise_tic(data, percent):
  l = len(data)
  noise_per = round(percent * l)
  ran = list(range(l))
  data_n = []
  rand_samp = random.sample(ran, noise_per)
  for i in rand_samp:
    lab = data[i,-1]
    if lab == 'negative':
      lab = 'positive'
    else:
      lab = 'negative'
    
    data_n.append(deepcopy(data[i,:]))
    data[i,-1] = lab

  data_r = np.append(data, data_n, axis=0)
  np.random.shuffle(data_r)

  return data_r

# Add Misclassification Class Noise to Wine Dataset
def Add_misclass_noise_wine(data, percent):
  l = len(data)
  noise_per = round(percent * l)
  ran = list(range(l))
  rand_samp = random.sample(ran, noise_per)
  for i in rand_samp:
    lab = data[i,0]
    if lab == 1:
      lab = random.choice([2,3])
    elif lab == 2:
      lab = random.choice([1,3])
    elif lab == 3:
      lab =random.choice([1,2])
    
    data[i,0] = lab

  return data

# Add Misclassification Class Noise to Tic-Tac Dataset
def Add_misclass_noise_tic(data, percent):
  l = len(data)
  noise_per = round(percent * l)
  ran = list(range(l))
  rand_samp = random.sample(ran, noise_per)
  for i in rand_samp:
    lab = data[i,-1]
    if lab == 'negative':
      lab = 'positive'
    else:
      lab = 'negative'
    
    data[i,-1] = lab

  return data


# Calculate Accuracy of ID3 classification based on type and amount of noise, 
# Using 10 times 10 Fold Validation
# Returns Accuracy of CLassifier based on both, Info Gain and Gain Ratio
# noise: CvC = 0, DvC = 1, CvD = 2, DvD = 3, Contradictory = 4, Misclassification = 5

def Cacl_accuracy(data, lab_col_num, continuous, noise_type = 0, noise=0):

  rkf = RepeatedKFold(n_splits=10, n_repeats=10)#, random_state=2652124)
  count=0
  accu_list = []
  accu_list_GR = []

  best_acc = 0
  best_acc_GR = 0
 
  for train_index, test_index in rkf.split(data):

    predictions = []
    predictions_GR = []
    count+=1
    train_data, test_data = data[train_index, :], data[test_index, :]
    scaler = preprocessing.StandardScaler()

    if noise_type!=0:
      if continuous == 1:
        if noise_type == 1:
          train_lab = deepcopy(train_data[:,0])
          train_data = scaler.fit_transform( train_data )
          train_data[:, 0] = train_lab

          train_data = Add_noise_cont(train_data, noise)
       
        # Scale Test data As well
          test_lab = deepcopy(test_data[:,0])
          test_data = scaler.transform( test_data )
          test_data[:, 0] = test_lab

        elif noise_type == 2: 

          # Scale Train data As well
          train_lab = deepcopy(train_data[:,0])
          train_data = scaler.fit_transform( train_data )
          train_data[:, 0] = train_lab
          
          
          test_lab = deepcopy(test_data[:,0])
          test_data = scaler.transform( test_data )
          test_data[:, 0] = test_lab

          test_data = Add_noise_cont(test_data, noise)

        elif noise_type == 3:
          train_lab = deepcopy(train_data[:,0])
          train_data = scaler.fit_transform( train_data )
          train_data[:, 0] = train_lab

          train_data = Add_noise_cont(train_data, noise) 

          test_lab = deepcopy(test_data[:,0])
          test_data = scaler.transform( test_data )
          test_data[:, 0] = test_lab   
          
          test_data = Add_noise_cont(test_data, noise)
    
        elif noise_type == 4:
          train_data = Add_contrad_noise_wine(train_data, noise)      
        elif noise_type == 5:
          train_data = Add_misclass_noise_wine(train_data, noise)
      else:
        if noise_type == 1:
          train_data = Add_noise_cat(train_data, noise)
        elif noise_type == 2: 
          test_data = Add_noise_cat(test_data, noise)
        elif noise_type == 3:
          train_data = Add_noise_cat(train_data, noise)      
          test_data = Add_noise_cat(test_data, noise)
        elif noise_type == 4:
          train_data = Add_contrad_noise_tic(train_data, noise)      
        elif noise_type == 5:
          train_data = Add_misclass_noise_tic(train_data, noise)

    root_t = ID3([], train_data, label_col_num= lab_col_num, dat_type=continuous, curr_dep = 0)
    root_t_GR = ID3_gain_ratio([], train_data, label_col_num=lab_col_num, dat_type=continuous, curr_dep = 0)    
    
    correct = 0
    correct_GR = 0

    len_test = len(test_data)
    
    if continuous==0:
      for i in range(len_test):
        pr = predict_tic(root_t, test_data[i, :])
        if pr == test_data[i, lab_col_num]:
          correct +=1
        predictions.append(pr)      
        pr1 = predict_tic(root_t_GR, test_data[i, :])
        if pr1 == test_data[i, lab_col_num]:
          correct_GR +=1
        predictions_GR.append(pr1)
    else:
      for i in range(len_test):
        pr = predict(root_t, test_data[i, :])
        if pr == test_data[i, lab_col_num]:
          correct +=1
        predictions.append(pr)      
        pr1 = predict(root_t_GR, test_data[i, :])
        if pr1 == test_data[i, lab_col_num]:
          correct_GR +=1
        predictions_GR.append(pr1) 

    accuracy = correct/len_test
   
    if accuracy >= best_acc:
      best_acc = accuracy
      best_pred = [deepcopy(predictions),deepcopy(list(test_data[:, lab_col_num]))]
   
    accu_list.append(accuracy)

    accuracy_GR = correct_GR/len_test
    
    if accuracy_GR >= best_acc_GR:
      best_acc_GR = accuracy_GR
      best_pred_GR = [deepcopy(predictions_GR),deepcopy(list(test_data[:, lab_col_num]))]
    
    accu_list_GR.append(accuracy_GR)

  return accu_list, accu_list_GR, best_pred, best_pred_GR, root_t

def Q2():

  print("\nCalculating Accuracy of Tic-Tac Dataset...")
  
  tic_accuracy, tic_accuracy_GR, tic_pred, tic_pred_GR, root_t = Cacl_accuracy(TIC_TAC_DATA, lab_col_num=-1, continuous=0 )
 
  print("Mean Tic-Tac Accuracy = ", mean(tic_accuracy))
  print("Mean Tic-Tac Variance = ", np.var(tic_accuracy))

  print("Mean Tic-Tac Accuracy Gain Ratio = ", mean(tic_accuracy_GR))
  print("Mean Tic-Tac Variance Gain Ratio = ", np.var(tic_accuracy_GR))
 
  print("\nCONFUSION MATRIX TIC-TAC")
  print(confusion_matrix(tic_pred[1], tic_pred[0], labels=['positive', 'negative']))
  print("\nCONFUSION MATRIX TIC-TAC Gain Ratio")
  print(confusion_matrix(tic_pred_GR[1], tic_pred_GR[0], labels=['positive', 'negative']))

  print("\nCalculating Accuracy of Wine Dataset...")

  wine_accuracy, wine_accuracy_GR, wine_pred, wine_pred_GR, dummy = Cacl_accuracy(WINE_DATA, lab_col_num=0, continuous=1 )
  
  print("Mean Wine Accuracy = ", mean(wine_accuracy))
  print("Mean Wine Variance = ", np.var(wine_accuracy))
  print("Mean Wine Accuracy Gain Ratio = ", mean(wine_accuracy_GR))
  print("Mean Wine Variance Gain Ratio = ", np.var(wine_accuracy_GR))

  print("\nCONFUSION MATRIX WINE")
  print(confusion_matrix(wine_pred[1], wine_pred[0], labels=[1,2,3]))
  print("\nCONFUSION MATRIX WINE Gain Ratio")
  print(confusion_matrix(wine_pred_GR[1], wine_pred_GR[0], labels=[1,2,3]))


# noise: CvC = 0, DvC = 1, CvD = 2, DvD = 3, Contradictory = 4, Misclassification = 5
def Q3_A_wine():
  
  print("\nCalculating & Plotting Effect of Attribute Noise on Wine Dataset...")
  n = [0.05, 0.1, 0.15]
  wine_accuracy_n0, wine_accuracy_GR, wine_pred, wine_pred_GR, dummy = Cacl_accuracy(WINE_DATA, lab_col_num=0, continuous=1, noise_type=0 )
  CvC = [[mean(wine_accuracy_GR), np.var(wine_accuracy_GR)]]*3
  DvC = []
  CvD = []
  DvD = []
  for i in n: 
    wine_accuracy_n, wine_accuracy_GR1, wine_pred, wine_pred_GR, dummy = Cacl_accuracy(WINE_DATA, lab_col_num=0, continuous=1, noise_type=1, noise=i)
    wine_accuracy_n, wine_accuracy_GR2, wine_pred, wine_pred_GR, dummy = Cacl_accuracy(WINE_DATA, lab_col_num=0, continuous=1, noise_type=2, noise=i)    
    wine_accuracy_n, wine_accuracy_GR3, wine_pred, wine_pred_GR, dummy = Cacl_accuracy(WINE_DATA, lab_col_num=0, continuous=1, noise_type=3, noise=i)

    DvC.append([mean(wine_accuracy_GR1), np.var(wine_accuracy_GR1)])
    CvD.append([mean(wine_accuracy_GR2), np.var(wine_accuracy_GR2)])
    DvD.append([mean(wine_accuracy_GR3), np.var(wine_accuracy_GR3)])  
  
  CvC = np.array(CvC)
  DvC = np.array(DvC)
  CvD = np.array(CvD)
  DvD = np.array(DvD)

  print('### Wine Data ####\n')
  print('CvC Accuracy/Variance: ', CvC[0,0], '/', CvC[0, 1],'\n')
  print('DvC Accuracy/Variance:\n 5% noise: ', DvC[0,0],'/', DvC[0,1],'\n 10% noise: ', DvC[1,0],'/', DvC[1,1],'\n 15% noise: ', DvC[2,0],'/', DvC[2,1],'\n')
  print('CvD Accuracy/Variance:\n 5% noise: ', CvD[0,0],'/', CvD[0,1],'\n 10% noise: ', CvD[1,0],'/', CvD[1,1],'\n 15% noise: ', CvD[2,0],'/', CvD[2,1],'\n')
  print('DvD Accuracy/Variance:\n 5% noise: ', DvD[0,0],'/', DvD[0,1],'\n 10% noise: ', DvD[1,0],'/', DvD[1,1],'\n 15% noise: ', DvD[2,0],'/', DvD[2,1],'\n')

  plt.figure()
  plt.plot([5, 10, 15], CvC[:,0], label = "CvC")
  plt.plot([5, 10, 15], DvC[:,0], label = "DvC")
  plt.plot([5, 10, 15], CvD[:,0], label = "CvD")
  plt.plot([5, 10, 15], DvD[:,0], label = "DvD")

  plt.ylabel("Accuracy")
  plt.xlabel("Noise %")
  plt.title("Wine Data")
  plt.legend(fancybox=True, framealpha=0.5)
  plt.show()


# noise: CvC = 0, DvC = 1, CvD = 2, DvD = 3, Contradictory = 4, Misclassification = 5
def Q3_A_tic():

  print("\nCalculating & Plotting Effect of Attribute Noise on Tic-Tac Dataset...")
  n = [0.05, 0.1, 0.15]
  tic_accuracy, tic_accuracy_GR, tic_pred, tic_pred_GR, dummy = Cacl_accuracy(TIC_TAC_DATA, lab_col_num=-1, continuous=0, noise_type=0 )
  CvC = [[mean(tic_accuracy_GR), np.var(tic_accuracy_GR)]]*3
  DvC = []
  CvD = []
  DvD = []
  for i in n: 
    tic_accuracy_n, tic_accuracy_GR1, tic_pred, tic_pred_GR, dummy = Cacl_accuracy(TIC_TAC_DATA, lab_col_num=-1, continuous=0, noise_type=1, noise=i)
    tic_accuracy_n, tic_accuracy_GR2, tic_pred, tic_pred_GR, dummy = Cacl_accuracy(TIC_TAC_DATA, lab_col_num=-1, continuous=0, noise_type=2, noise=i)    
    tic_accuracy_n, tic_accuracy_GR3, tic_pred, tic_pred_GR, dummy = Cacl_accuracy(TIC_TAC_DATA, lab_col_num=-1, continuous=0, noise_type=3, noise=i)

    DvC.append([mean(tic_accuracy_GR1), np.var(tic_accuracy_GR1)])
    CvD.append([mean(tic_accuracy_GR2), np.var(tic_accuracy_GR2)])
    DvD.append([mean(tic_accuracy_GR3), np.var(tic_accuracy_GR3)])  
  
  CvCt = np.array(CvC)
  DvCt = np.array(DvC)
  CvDt = np.array(CvD)
  DvDt = np.array(DvD)
  
  print('### Tic-Tac Data ####\n')
  print('CvC Accuracy/Variance: ', CvCt[0,0], '/', CvCt[0, 1],'\n')
  print('DvC Accuracy/Variance:\n 5% noise: ', DvCt[0,0],'/', DvCt[0,1],'\n 10% noise: ', DvCt[1,0],'/', DvCt[1,1],'\n 15% noise: ', DvCt[2,0],'/', DvCt[2,1],'\n')
  print('CvD Accuracy/Variance:\n 5% noise: ', CvDt[0,0],'/', CvDt[0,1],'\n 10% noise: ', CvDt[1,0],'/', CvDt[1,1],'\n 15% noise: ', CvDt[2,0],'/', CvDt[2,1],'\n')
  print('DvD Accuracy/Variance:\n 5% noise: ', DvDt[0,0],'/', DvDt[0,1],'\n 10% noise: ', DvDt[1,0],'/', DvDt[1,1],'\n 15% noise: ', DvDt[2,0],'/', DvDt[2,1],'\n')

  plt.figure()
  plt.plot([5, 10, 15], CvCt[:,0], label = "CvC")
  plt.plot([5, 10, 15], DvCt[:,0], label = "DvC")
  plt.plot([5, 10, 15], CvDt[:,0], label = "CvD")
  plt.plot([5, 10, 15], DvDt[:,0], label = "DvD")

  plt.ylabel("Accuracy")
  plt.xlabel("Noise %")
  plt.title("Tic-Tac Data")
  plt.legend(fancybox=True, framealpha=0.5)
  plt.show()

# noise: CvC = 0, DvC = 1, CvD = 2, DvD = 3, Contradictory = 4, Misclassification = 5
def Q3_B_wine():
  
  print("\nCalculating & Plotting Effect of Class Noise on Wine Dataset...")
  n = [0.05, 0.1, 0.15]
  contradict = []
  misclass = []
  wine_accuracy_n0, wine_accuracy_GR, wine_pred, wine_pred_GR, dummy = Cacl_accuracy(WINE_DATA, lab_col_num=0, continuous=1, noise_type=0 )
  no_noise_wine = [[mean(wine_accuracy_GR), np.var(wine_accuracy_GR)]]*3
  for i in n: 
    wine_accuracy_n, wine_accuracy_GR1, wine_pred, wine_pred_GR, dummy = Cacl_accuracy(WINE_DATA, lab_col_num=0, continuous=1, noise_type=4, noise=i)
    wine_accuracy_n, wine_accuracy_GR2, wine_pred, wine_pred_GR, dummy = Cacl_accuracy(WINE_DATA, lab_col_num=0, continuous=1, noise_type=5, noise=i)    

    contradict.append([mean(wine_accuracy_GR1), np.var(wine_accuracy_GR1)])
    misclass.append([mean(wine_accuracy_GR2), np.var(wine_accuracy_GR2)])
  
  no_noise_wine = np.array(no_noise_wine)
  contrad_wine = np.array(contradict)
  misclass_wine = np.array(misclass)


  print('### Wine Data ####\n')
  print('No Noise Accuracy/Variance: ', no_noise_wine[0,0], '/', no_noise_wine[0, 1],'\n')
  print('Contradiction Accuracy/Variance:\n 5% noise: ', contrad_wine[0,0],'/', contrad_wine[0,1],'\n 10% noise: ', contrad_wine[1,0],'/', contrad_wine[1,1],
        '\n 15% noise: ', contrad_wine[2,0],'/', contrad_wine[2,1],'\n')
  print('Misclassification Accuracy/Variance:\n 5% noise: ', misclass_wine[0,0],'/', misclass_wine[0,1],'\n 10% noise: ', misclass_wine[1,0],'/', misclass_wine[1,1],
        '\n 15% noise: ', misclass_wine[2,0],'/', misclass_wine[2,1],'\n')

  plt.figure()
  plt.plot([5, 10, 15], no_noise_wine[:,0], label = "No Noise")
  plt.plot([5, 10, 15], contrad_wine[:,0], label = "Contradiction")
  plt.plot([5, 10, 15], misclass_wine[:,0], label = "Misclassification")

  plt.ylabel("Accuracy")
  plt.xlabel("Noise %")
  plt.title("Wine Data")
  plt.legend(fancybox=True, framealpha=0.5)
  plt.show()


def Q3_B_tic():
  
  print("\nCalculating & Plotting Effect of Class Noise on Tic-Tac Dataset...")

  n = [0.05, 0.1, 0.15]
  contradict = []
  misclass = []
  tic_accuracy, tic_accuracy_GR, tic_pred, tic_pred_GR, dummy = Cacl_accuracy(TIC_TAC_DATA, lab_col_num=-1, continuous=0, noise_type=0 )
  no_noise_tic = [[mean(tic_accuracy_GR), np.var(tic_accuracy_GR)]]*3
  
  for i in n: 
    tic_accuracy_n, tic_accuracy_GR1, tic_pred, tic_pred_GR, dummy = Cacl_accuracy(TIC_TAC_DATA, lab_col_num=-1, continuous=0, noise_type=4, noise=i)
    tic_accuracy_n, tic_accuracy_GR2, tic_pred, tic_pred_GR, dummy = Cacl_accuracy(TIC_TAC_DATA, lab_col_num=-1, continuous=0, noise_type=5, noise=i)    

    contradict.append([mean(tic_accuracy_GR1), np.var(tic_accuracy_GR1)])
    misclass.append([mean(tic_accuracy_GR2), np.var(tic_accuracy_GR2)])
  
  no_noise_tic = np.array(no_noise_tic)
  contrad_tic = np.array(contradict)
  misclass_tic = np.array(misclass)

  print('### Tic-Tac Data ####\n')
  print('No Noise Accuracy/Variance: ', no_noise_tic[0,0], '/', no_noise_tic[0, 1],'\n')
  print('Contradiction Accuracy/Variance:\n 5% noise: ', contrad_tic[0,0],'/', contrad_tic[0,1],'\n 10% noise: ', contrad_tic[1,0],'/', contrad_tic[1,1],
        '\n 15% noise: ', contrad_tic[2,0],'/', contrad_tic[2,1],'\n')
  print('Misclassification Accuracy/Variance:\n 5% noise: ', misclass_tic[0,0],'/', misclass_tic[0,1],'\n 10% noise: ', misclass_tic[1,0],'/', misclass_tic[1,1],
        '\n 15% noise: ', misclass_tic[2,0],'/', misclass_tic[2,1],'\n')

  plt.figure()
  plt.plot([5, 10, 15], no_noise_tic[:,0], label = "No Noise")
  plt.plot([5, 10, 15], contrad_tic[:,0], label = "Contradiction")
  plt.plot([5, 10, 15], misclass_tic[:,0], label = "Misclassification")

  plt.ylabel("Accuracy")
  plt.xlabel("Noise %")
  plt.title("Tic-Tac Data")
  plt.legend(fancybox=True, framealpha=0.5)
  plt.show()

def main():

  Q2()
  Q3_A_wine()
  Q3_A_tic()
  Q3_B_wine()
  Q3_B_tic()


if __name__== "__main__":
  main()
